{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jacobgokul/ML-Playground/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxHJgh7ViH61"
      },
      "source": [
        "# What is NLP?\n",
        "Natural Language Processing (NLP) is a field in AI that helps computers understand, interpret, and generate human language.\n",
        "\n",
        "✅ It's how machines understand text and speech, like Google Translate or Siri or Alexa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALEm2Bx8iUsr"
      },
      "source": [
        "# Why NLP Matters\n",
        "Humans speak in natural languages like English, Tamil, or Hindi, but computers understand only numbers. NLP acts as the translator between human language and machine language.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSPoJrnLin7Y"
      },
      "source": [
        "# What Exactly Does NLP Do?\n",
        "\n",
        "| Task                    | Example                                             |\n",
        "| ----------------------- | --------------------------------------------------- |\n",
        "| **Understand Meaning**  | Understand “I’m feeling down” means someone is sad  |\n",
        "| **Extract Information** | Pull names, dates, locations from articles (NER)    |\n",
        "| **Translate Languages** | Convert English → Japanese using translation models |\n",
        "| **Generate Text**       | Write a paragraph or code based on your input       |\n",
        "| **Summarize Documents** | Condense a 2000-word article into 3 lines           |\n",
        "| **Answer Questions**    | Like ChatGPT does                                   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA4l5BfCjNG3"
      },
      "source": [
        "# Key Concepts | Techniques\n",
        "\n",
        "## 1. Tokenization\n",
        "\n",
        "- What: Splits sentences into words or subwords.\n",
        "\n",
        "- Why: ML models can’t understand full text – they need units (tokens).\n",
        "\n",
        "- Types:\n",
        "\n",
        "    - Word tokenization → [\"Hello\", \"world\"]\n",
        "\n",
        "    - Character tokenization → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
        "\n",
        "    - Subword tokenization (used in Transformers) → \"playing\" → [\"play\", \"##ing\"]\n",
        "\n",
        "    ```py\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    word_tokenize(\"I'm learning NLP.\")\n",
        "    # ['I', \"'m\", 'learning', 'NLP', '.']\n",
        "    ```\n",
        "\n",
        "## 2. Stopword Removal\n",
        "- What: Remove frequent/common words like the, is, a, an.\n",
        "\n",
        "- Why: These words occur a lot but carry little meaning in classification tasks.\n",
        "    \n",
        "    ```py\n",
        "    from nltk.corpus import stopwords\n",
        "    stopwords.words('english')  # includes 'is', 'the', etc.\n",
        "    ```\n",
        "\n",
        "## 3. Stemming vs Lemmatization\n",
        "| Stemming            | Lemmatization               |\n",
        "| ------------------- | --------------------------- |\n",
        "| Cuts suffix         | Finds proper root word      |\n",
        "| “studies” → “studi” | “studies” → “study”         |\n",
        "| Less accurate       | More linguistically correct |\n",
        "\n",
        "Steamming:\n",
        " Running -> remove 'ing' -> Runn\n",
        "\n",
        "Lemmatization:\n",
        " Running -> root word -> Run\n",
        "\n",
        "```py\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(stemmer.stem(\"studies\"))   # studi\n",
        "print(lemmatizer.lemmatize(\"studies\"))  # study\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Bag of words (BoW)\n",
        "- It’s the simplest way to turn text into numbers so a machine can use it.\n",
        "- Idea: “Don’t care about order of words, just count how many times each word appears.”\n",
        "- Called a bag because it’s like putting all words into a bag, shaking it, and just counting—not worrying about grammar or sequence.\n",
        "\n",
        "#### Step-by-step example\n",
        "- Imagine two sentences:\n",
        "\n",
        "    - “I love NLP”\n",
        "\n",
        "    - “I love Python”\n",
        "\n",
        "- Step 1: Build Vocabulary\n",
        "    - Collect all unique words → [“I”, “love”, “NLP”, “Python”]\n",
        "\n",
        "- Step 2: Represent each sentence as a vector (word counts)\n",
        "    - Sentence 1: “I love NLP” → [1, 1, 1, 0]\n",
        "    - Sentence 2: “I love Python” → [1, 1, 0, 1]\n",
        "\n",
        "Each number says how many times that word appears.\n",
        "\n",
        "#### Pros and Cons\n",
        "- Advantages\n",
        "    - Super simple, easy to understand.\n",
        "\n",
        "    - Works well for small tasks (spam detection, simple text classification).\n",
        "\n",
        "- Disadvantages\n",
        "    - Ignores order → “dog bites man” vs “man bites dog” look the same.\n",
        "\n",
        "    - Vocabulary grows huge as text grows (sparse matrix).\n",
        "\n",
        "    - Doesn’t capture meaning or similarity (e.g., “happy” ≠ “glad”).\n",
        "\n",
        "\n",
        "```py\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Example documents\n",
        "docs = [\"I love NLP\", \"I love Python\"]\n",
        "\n",
        "# Create BoW model\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(docs)\n",
        "\n",
        "# Show vocabulary\n",
        "print(cv.get_feature_names_out())\n",
        "# ['love', 'nlp', 'python']\n",
        "\n",
        "# Show BoW vectors\n",
        "print(X.toarray())\n",
        "# [[1, 1, 0],\n",
        "#  [1, 0, 1]]\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "What: Improves BoW by reducing the weight of common words.\n",
        "\n",
        "Why: Words like \"good\", \"the\", \"very\" may appear in every document, but we want to focus on rare but important terms.\n",
        "### TF\n",
        "- Measures how often a term appears in a document.\n",
        "- So, repetitive words in a document will have a high TF score.\n",
        "#### Formula\n",
        "- TF = Count of term t in document d​ / Total terms in document d\n",
        "\n",
        "##### Example:\n",
        "    - Document = \"I love NLP, I love Python\"\n",
        "        - Word “love” count = 2\n",
        "        - Total words = 6\n",
        "        - TF(love) = 2/6 = 0.33\n",
        "\n",
        "### IDF\n",
        "- Measures how rare a term is across all documents.\n",
        "- So, common terms across many documents (like “the”, “is”) will have a low IDF score.\n",
        "- Rare terms (that appear in few documents) will have a high IDF score.\n",
        "\n",
        "#### Formula\n",
        "- IDF = log(Total number of documents /  Number of docs containing the word)\n",
        "\n",
        "##### Example:\n",
        "    - Suppose we have 10 documents.\n",
        "\n",
        "    - Word “Python” appears in 2 of them.\n",
        "\n",
        "    - IDF(Python) = log(10 / 2) = log(5) ≈ 1.6\n",
        "\n",
        "    - Word “the” appears in all 10 docs → log(10/10) = 0\n",
        "\n",
        "#### Formula for TF-IDF\n",
        "    TF-IDF(t,d)=TF(t,d)×IDF(t)\n",
        "\n",
        "- High TF-IDF: Term appears frequently in a specific doc, but rarely in others.\n",
        "\n",
        "- Low TF-IDF: Term is either common in all docs or infrequent in the target doc.\n",
        "\n",
        "\n",
        "##### Code:\n",
        "```py\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "docs = [\"I love NLP\", \"NLP loves me\", \"I love Python and NLP\"]\n",
        "\n",
        "# Create TF-IDF model\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(docs)\n",
        "\n",
        "# Vocabulary\n",
        "print(tfidf.get_feature_names_out())\n",
        "\n",
        "# TF-IDF values\n",
        "print(X.toarray())\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gseKRRNh9a8"
      },
      "source": [
        "## 6. Word Embeddings\n",
        "\n",
        "### What are Word Embeddings?\n",
        "Word embeddings are a way of representing words as dense vectors (lists of numbers) where:\n",
        "    - Words with similar meaning are closer in vector space.\n",
        "\n",
        "    - Captures semantics (meaning), unlike BoW or TF-IDF.\n",
        "\n",
        "Think of it as giving each word an address in a multi-dimensional space. Words that “live” near each other mean similar things.\n",
        "\n",
        "### Why not BoW/TF-IDF?\n",
        "- **BoW/TF-IDF** → Just counts. Doesn’t know that “king” and “queen” are related.\n",
        "\n",
        "- **Embeddings** → Learn from context. So “dog” is close to “puppy,” but far from “car.”\n",
        "\n",
        "### Type of Embeddings\n",
        "- Word2Vec \n",
        "    - Introduced by Google in 2013\n",
        "    - Trained on huge text.\n",
        "    - Captures famous analogy:\n",
        "        king - man + woman ≈ queen\n",
        "- GloVe\n",
        "    - Stanford in 2014\n",
        "    - Uses global word co-occurrence.\n",
        "    - Learns “statistics of the whole corpus.”\n",
        "- FastText\n",
        "    - Facebook by 2016\n",
        "    - Works with subwords (handles rare words, misspellings better).\n",
        "- BERT embeddings\n",
        "    - Google by 2018\n",
        "    -  Contextual embeddings → word meaning depends on sentence.\n",
        "    - Example:\n",
        "        - “I sat by the bank of the river.”\n",
        "        - “I went to the bank to deposit money.”\n",
        "        Same word → different embeddings.\n",
        "\n",
        "```py\n",
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Training a small toy model\n",
        "sentences = [\n",
        "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
        "    [\"I\", \"love\", \"deep\", \"learning\"],\n",
        "    [\"NLP\", \"is\", \"fun\"],\n",
        "    [\"Python\", \"is\", \"great\", \"for\", \"NLP\"]\n",
        "]\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=20, window=3, min_count=1)\n",
        "\n",
        "# Vector for word 'NLP'\n",
        "print(model.wv['NLP'])\n",
        "\n",
        "# Find similar words\n",
        "print(model.wv.most_similar('love'))\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMMWPKIMPsQPP/CZQzYQ3jx",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
