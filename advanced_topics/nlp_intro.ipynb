{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMWPKIMPsQPP/CZQzYQ3jx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jacobgokul/ML-Playground/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is NLP?\n",
        "Natural Language Processing (NLP) is a field in AI that helps computers understand, interpret, and generate human language.\n",
        "\n",
        "✅ It's how machines understand text and speech, like Google Translate or Siri or Alexa."
      ],
      "metadata": {
        "id": "OxHJgh7ViH61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why NLP Matters\n",
        "Humans speak in natural languages like English, Tamil, or Hindi, but computers understand only numbers. NLP acts as the translator between human language and machine language.\n",
        "\n"
      ],
      "metadata": {
        "id": "ALEm2Bx8iUsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Exactly Does NLP Do?\n",
        "\n",
        "| Task                    | Example                                             |\n",
        "| ----------------------- | --------------------------------------------------- |\n",
        "| **Understand Meaning**  | Understand “I’m feeling down” means someone is sad  |\n",
        "| **Extract Information** | Pull names, dates, locations from articles (NER)    |\n",
        "| **Translate Languages** | Convert English → Japanese using translation models |\n",
        "| **Generate Text**       | Write a paragraph or code based on your input       |\n",
        "| **Summarize Documents** | Condense a 2000-word article into 3 lines           |\n",
        "| **Answer Questions**    | Like ChatGPT does                                   |\n"
      ],
      "metadata": {
        "id": "YSPoJrnLin7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Concepts | Techniques\n",
        "\n",
        "## 1. Tokenization\n",
        "\n",
        "What: Splits sentences into words or subwords.\n",
        "\n",
        "Why: ML models can’t understand full text – they need units (tokens).\n",
        "\n",
        "Types:\n",
        "\n",
        "Word tokenization → [\"Hello\", \"world\"]\n",
        "\n",
        "Character tokenization → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
        "\n",
        "Subword tokenization (used in Transformers) → \"playing\" → [\"play\", \"##ing\"]\n",
        "\n",
        "```\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"I'm learning NLP.\")\n",
        "# ['I', \"'m\", 'learning', 'NLP', '.']\n",
        "```\n",
        "\n",
        "## 2. Stopword Removal\n",
        "What: Remove frequent/common words like the, is, a, an.\n",
        "\n",
        "Why: These words occur a lot but carry little meaning in classification tasks.\n",
        "```\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')  # includes 'is', 'the', etc.\n",
        "```\n",
        "\n",
        "## 3. Stemming vs Lemmatization\n",
        "| Stemming            | Lemmatization               |\n",
        "| ------------------- | --------------------------- |\n",
        "| Cuts suffix         | Finds proper root word      |\n",
        "| “studies” → “studi” | “studies” → “study”         |\n",
        "| Less accurate       | More linguistically correct |\n",
        "\n",
        "Steamming:\n",
        " Running -> remove 'ing' -> Runn\n",
        "\n",
        "Lemmatization:\n",
        " Running -> root word -> Run\n",
        "\n",
        "## 4. Bag of Words (BoW)\n",
        "Each word is treated like a feature in a vector.\n",
        "```\n",
        "Vocabulary: [‘I’, ‘love’, ‘NLP’]\n",
        "Sentence: \"I love NLP\" → [1,1,1]\n",
        "Sentence: \"I love Python\" → [1,1,0]\n",
        "```\n",
        "\n",
        "## 5. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "What: Improves BoW by reducing the weight of common words.\n",
        "Why: Words like \"good\", \"the\", \"very\" may appear in every document, but we want to focus on rare but important terms.\n",
        "### TF\n",
        "- Measures how often a term appears in a document.\n",
        "- So, repetitive words in a document will have a high TF score.\n",
        "#### Formula\n",
        "- TF = frequency of word in doc\n",
        "\n",
        "### IDF\n",
        "- Measures how rare a term is across all documents.\n",
        "- So, common terms across many documents (like “the”, “is”) will have a low IDF score.\n",
        "- Rare terms (that appear in few documents) will have a high IDF score.\n",
        "\n",
        "#### Formula\n",
        "- IDF = log(total docs / docs containing the word)\n",
        "\n",
        "#### Formula for TF-IDF\n",
        "    TF-IDF(t,d)=TF(t,d)×IDF(t)\n",
        "\n",
        "- High TF-IDF: Term appears frequently in a specific doc, but rarely in others.\n",
        "\n",
        "- Low TF-IDF: Term is either common in all docs or infrequent in the target doc.\n",
        "\n",
        "```\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform([\"I love NLP\", \"NLP loves me\"])\n",
        "```\n",
        "\n",
        "## 6. Word Embeddings\n",
        " What: Map each word to a dense vector that captures meaning and context.\n",
        " Why: Unlike BoW/TF-IDF, embeddings like Word2Vec or GloVe understand similarity.\n",
        "\n",
        " ### Popular Embedding Techniques:\n",
        "    Word2Vec\n",
        "\n",
        "    GloVe\n",
        "\n",
        "    FastText\n",
        "\n",
        "    BERT embeddings (contextual)\n",
        "  \n",
        "\n",
        "## 7. Named Entity Recognition (NER)\n",
        "- Extract entities like names, locations, dates etc,.\n",
        "-  Useful in chatbots, search engines, info extraction.\n",
        "\n",
        "Example:\n",
        "'Gokul works in Accenture at Coimbatore campus'\n",
        "-> PERSON: Gokul, ORG: Accenture, LOCATION: Coimbatore"
      ],
      "metadata": {
        "id": "vA4l5BfCjNG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gseKRRNh9a8"
      },
      "outputs": [],
      "source": []
    }
  ]
}