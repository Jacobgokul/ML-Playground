{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDPH5ItnDpNclDUUBiTXax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jacobgokul/ML-Playground/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Reinforcement Learning (RL)?\n",
        "\n",
        "It's a type of learning where an agent learns to make decisions by interacting with an environment and getting rewards or penalties.\n",
        "\n",
        "## Key Idea:\n",
        "\n",
        "Just like a kid learning to ride a cycle:\n",
        "\n",
        "- Try something â†’ fall â†’ learn\n",
        "\n",
        "- Try again â†’ better balance â†’ rewarded\n",
        "\n",
        "In RL:\n",
        "\n",
        "- Agent = the learner (e.g., AI bot)\n",
        "\n",
        "- Environment = the world it interacts with\n",
        "\n",
        "- Action = what the agent does\n",
        "\n",
        "- Reward = feedback (positive or negative)\n",
        "\n",
        "- Policy = the strategy used by the agent\n",
        "\n",
        "- Episode = one complete interaction cycle\n",
        "\n",
        "\n",
        "\n",
        "## Real Example â€“ Game AI\n",
        "\n",
        "Letâ€™s say an AI is learning to play a car racing game:\n",
        "\n",
        "Every time it stays on track = +1 point\n",
        "\n",
        "If it goes off road = -5 points\n",
        "\n",
        "If it completes a lap = +10 points\n",
        "\n",
        "The AI will try different strategies to maximize total reward. Over time, it learns what actions lead to better results.\n",
        "\n",
        "\n",
        "## How RL Works (Simplified Flow):\n",
        "- Agent observes the state of the environment\n",
        "\n",
        "- Takes an action\n",
        "\n",
        "- Environment gives a reward and updates the state\n",
        "\n",
        "- Agent learns and improves its policy\n",
        "\n",
        "This loop keeps running until the agent becomes good at the task.\n",
        "\n",
        "\n",
        "ðŸ“‚ Types of Reinforcement Learning\n",
        "\n",
        "âœ… 1. Model-Free (No idea how the environment works)\n",
        "\n",
        "Learns only from experience\n",
        "\n",
        "Most common in games & robotics\n",
        "\n",
        "ðŸ§  Algorithms:\n",
        "\n",
        "- Q-Learning (classic)\n",
        "\n",
        "- Deep Q-Networks (DQN) â€“ Q-learning with neural networks\n",
        "\n",
        "- SARSA\n",
        "\n",
        "âœ… 2. Model-Based (Learns or knows the environmentâ€™s rules)\n",
        "\n",
        "It can simulate or plan future actions\n",
        "\n",
        "ðŸ§  Algorithms:\n",
        "- Dyna-Q\n",
        "\n",
        "- Monte Carlo Tree Search (used in AlphaGo)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hWOFUHae5st3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "P84jCr8J__4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning Code Example\n",
        "\n",
        "## Problem Setup: Grid World (4x4)\n",
        "\n",
        "The agent starts at position (0,0)\n",
        "\n",
        "The goal is to reach the bottom-right corner (3,3)\n",
        "\n",
        "The agent can move: up, down, left, right\n",
        "\n",
        "Each move gives -1 reward\n",
        "\n",
        "Reaching the goal gives +10 reward\n",
        "\n",
        "Hitting walls just keeps it in place"
      ],
      "metadata": {
        "id": "R5vgReuTAB65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WKkx0pW5hCv",
        "outputId": "81d3d3d0-7539-4603-df4d-aa9a88d43739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! âœ…\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Grid size (4x4 matrix)\n",
        "n_rows = 4\n",
        "n_cols = 4\n",
        "\n",
        "# Actions\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "action_dict = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
        "\n",
        "# Q-table [state_row][state_col][action]\n",
        "q_table = np.zeros((n_rows, n_cols, len(actions)))\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.1       # learning rate\n",
        "gamma = 0.9       # discount factor\n",
        "epsilon = 0.2     # exploration factor\n",
        "episodes = 500\n",
        "\n",
        "# Reward function\n",
        "def get_reward(state):\n",
        "    if state == (3, 3):\n",
        "        return 10\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Environment transition\n",
        "def take_action(state, action):\n",
        "    row, col = state\n",
        "\n",
        "    if action == 'up':\n",
        "        row = max(row - 1, 0)\n",
        "    elif action == 'down':\n",
        "        row = min(row + 1, n_rows - 1)\n",
        "    elif action == 'left':\n",
        "        col = max(col - 1, 0)\n",
        "    elif action == 'right':\n",
        "        col = min(col + 1, n_cols - 1)\n",
        "\n",
        "    return (row, col)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(episodes):\n",
        "    state = (0, 0)\n",
        "\n",
        "    while state != (3, 3):  # Until it reaches the goal\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(actions)\n",
        "        else:\n",
        "            # Pick best action from Q-table\n",
        "            action = actions[np.argmax(q_table[state[0], state[1]])]\n",
        "\n",
        "        new_state = take_action(state, action)\n",
        "        reward = get_reward(new_state)\n",
        "\n",
        "        old_q = q_table[state[0], state[1], action_dict[action]]\n",
        "        next_max = np.max(q_table[new_state[0], new_state[1]])\n",
        "\n",
        "        # Q-learning formula\n",
        "        new_q = old_q + alpha * (reward + gamma * next_max - old_q)\n",
        "        q_table[state[0], state[1], action_dict[action]] = new_q\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "print(\"Training complete! âœ…\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBDK1RdjA9H2",
        "outputId": "ef0ecaef-9c11-4518-ad58-38aca0747d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whatâ€™s Going On?\n",
        "\n",
        "- q_table: Stores the value of each (state, action) pair\n",
        "\n",
        "- epsilon: Balances exploration vs exploitation\n",
        "\n",
        "- gamma: Remembers future rewards\n",
        "\n",
        "- alpha: Learning rate â€” how fast it learns"
      ],
      "metadata": {
        "id": "-i4A2xqIBKEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the path taken by the agent from (0,0) to (3,3)\n",
        "state = (0, 0)\n",
        "path = [state]\n",
        "\n",
        "while state != (3, 3):\n",
        "    # Choose the best action (greedy)\n",
        "    best_action_idx = np.argmax(q_table[state[0], state[1]])\n",
        "    best_action = actions[best_action_idx]\n",
        "\n",
        "    # Move to the next state\n",
        "    new_state = take_action(state, best_action)\n",
        "    path.append(new_state)\n",
        "\n",
        "    # Break if stuck (safety condition)\n",
        "    if new_state == state:\n",
        "        print(\"Agent is stuck! ðŸš§\")\n",
        "        break\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "# Print the path\n",
        "print(\"ðŸ Optimal path from (0,0) to (3,3):\")\n",
        "for step in path:\n",
        "    print(step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA_17SL7BISv",
        "outputId": "8c0658a4-69ed-4fcf-a237-bb7b909faf98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ Optimal path from (0,0) to (3,3):\n",
            "(0, 0)\n",
            "(1, 0)\n",
            "(1, 1)\n",
            "(2, 1)\n",
            "(3, 1)\n",
            "(3, 2)\n",
            "(3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UsHIz7aoBcCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}