<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>Deep Q-Network (DQN) - ML Playground</title>
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;color:#1e293b;line-height:1.7}
.top-bar{background:#0f172a;padding:14px 32px;display:flex;align-items:center;gap:16px;position:sticky;top:0;z-index:10}
.top-bar a{color:#94a3b8;text-decoration:none;font-size:14px;font-weight:600;transition:.15s}
.top-bar a:hover{color:#f1f5f9}
.top-bar .sep{color:#334155}
.top-bar .current{color:#60a5fa}
.container{max-width:900px;margin:0 auto;padding:40px 32px 80px}
h1{font-size:32px;font-weight:800;letter-spacing:-.8px;margin-bottom:8px;color:#0f172a}
.subtitle{font-size:16px;color:#64748b;margin-bottom:32px;border-bottom:2px solid #e2e8f0;padding-bottom:20px}
h2{font-size:22px;font-weight:700;margin:36px 0 14px;color:#0f172a;letter-spacing:-.3px}
h3{font-size:17px;font-weight:700;margin:24px 0 10px;color:#1e293b}
p{margin-bottom:14px;font-size:15px;color:#334155}
ul,ol{margin:0 0 16px 24px;font-size:15px;color:#334155}
li{margin-bottom:6px}
.card{background:#fff;border:1px solid #e2e8f0;border-radius:12px;padding:24px;margin:20px 0}
.card-title{font-size:14px;font-weight:700;color:#64748b;text-transform:uppercase;letter-spacing:.8px;margin-bottom:12px}
.formula{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:15px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.8}
.code{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:13px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.6}
.highlight{background:#eff6ff;border-left:4px solid #3b82f6;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}
.highlight p{margin:0;color:#1e40af;font-size:14px}
.warning{background:#fef3c7;border-left:4px solid #f59e0b;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}
.warning p{margin:0;color:#92400e;font-size:14px}
.grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:16px 0}
.grid-item{background:#fff;border:1px solid #e2e8f0;border-radius:10px;padding:18px}
.grid-item h4{font-size:14px;font-weight:700;margin-bottom:6px}
.grid-item p{font-size:13px;margin:0}
.tag{display:inline-block;padding:3px 10px;border-radius:20px;font-size:11px;font-weight:600;background:#f1f5f9;color:#475569;margin:2px}
.pros{color:#059669}.cons{color:#dc2626}
table{width:100%;border-collapse:collapse;margin:16px 0;font-size:14px}
th{background:#f1f5f9;padding:10px 14px;text-align:left;font-weight:700;border:1px solid #e2e8f0}
td{padding:10px 14px;border:1px solid #e2e8f0}
@media(max-width:700px){.container{padding:20px 16px 60px}.grid{grid-template-columns:1fr}h1{font-size:24px}}
</style>
</head>
<body>
<div class="top-bar">
<a href="../../index.html">ML Playground</a>
<span class="sep">/</span>
<span class="current">Deep Q-Network (DQN)</span>
</div>
<div class="container">

<h1>Deep Q-Network (DQN)</h1>
<p class="subtitle">Combining Q-Learning with deep neural networks to handle environments with large or continuous state spaces.</p>

<h2>From Q-Learning to DQN</h2>
<p>Q-Learning stores Q-values in a table. This works for small state spaces (like a 4x4 grid), but what about an Atari game with millions of possible screen pixels? You can't have a table that big. DQN replaces the Q-table with a neural network that approximates Q-values for any state.</p>

<div class="highlight"><p>Q-Table: lookup table mapping (state, action) → value. Works for small spaces.<br>DQN: neural network that takes a state as input and outputs Q-values for all actions. Works for any state space.</p></div>

<h2>Key Idea</h2>
<div class="formula">Q-Learning Table:
  Q[state][action] = value    (lookup from table)

DQN Neural Network:
  Q(state; θ) → [Q(a1), Q(a2), ..., Q(an)]    (predict with neural net)

The network learns parameters θ to approximate the Q-function.</div>

<h2>Two Key Innovations</h2>

<h3>1. Experience Replay</h3>
<p>Instead of training on experiences sequentially (which is correlated and unstable), store experiences in a replay buffer and sample random mini-batches for training. This breaks correlations and reuses data efficiently.</p>
<div class="formula">Replay Buffer: [(s, a, r, s'), (s, a, r, s'), ...]  (up to 1M transitions)

Each step:
  1. Store (state, action, reward, next_state) in buffer
  2. Sample random batch of 32-64 experiences
  3. Train network on this batch</div>

<h3>2. Target Network</h3>
<p>Using the same network for both predictions and targets causes instability. DQN uses two networks: a "policy" network (updated every step) and a "target" network (updated less frequently by copying weights).</p>
<div class="formula">Loss = (r + γ * max_a' Q_target(s', a'; θ⁻) - Q(s, a; θ))²

θ   = policy network weights (updated every step)
θ⁻  = target network weights (copied from θ every C steps)</div>

<h2>Algorithm</h2>
<div class="card">
<div class="card-title">DQN Training Loop</div>
<ol>
<li>Initialize replay buffer D with capacity N</li>
<li>Initialize policy network Q with random weights θ</li>
<li>Initialize target network Q_target with weights θ⁻ = θ</li>
<li>For each episode:
    <ol>
    <li>Get initial state s</li>
    <li>For each step:
        <ol>
        <li>With probability ε select random action (explore), otherwise a = argmax_a Q(s; θ) (exploit)</li>
        <li>Execute action a, observe reward r and next state s'</li>
        <li>Store (s, a, r, s') in replay buffer D</li>
        <li>Sample random mini-batch from D</li>
        <li>Compute target: y = r + γ * max_a' Q_target(s', a'; θ⁻)</li>
        <li>Update θ by minimizing (y - Q(s, a; θ))²</li>
        <li>Every C steps: copy θ → θ⁻</li>
        </ol>
    </li>
    </ol>
</li>
</ol>
</div>

<h2>Code Implementation</h2>
<div class="code">import numpy as np
import random
from collections import deque

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=10000)  # Replay buffer
        self.gamma = 0.95                   # Discount factor
        self.epsilon = 1.0                  # Exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001

        self.model = self._build_model()          # Policy network
        self.target_model = self._build_model()    # Target network
        self.update_target()

    def _build_model(self):
        from tensorflow.keras import Sequential
        from tensorflow.keras.layers import Dense
        model = Sequential([
            Dense(64, activation='relu', input_dim=self.state_size),
            Dense(64, activation='relu'),
            Dense(self.action_size, activation='linear')  # Q-values for each action
        ])
        model.compile(optimizer='adam', loss='mse')
        return model

    def update_target(self):
        # Copy policy network weights to target network
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        # Epsilon-greedy action selection
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        q_values = self.model.predict(state, verbose=0)
        return np.argmax(q_values[0])

    def replay(self, batch_size=32):
        # Train on random batch from replay buffer
        if len(self.memory) < batch_size:
            return
        batch = random.sample(self.memory, batch_size)

        for state, action, reward, next_state, done in batch:
            target = reward
            if not done:
                # Use TARGET network for stability
                target += self.gamma * np.max(self.target_model.predict(next_state, verbose=0)[0])

            target_f = self.model.predict(state, verbose=0)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Usage:
# agent = DQNAgent(state_size=4, action_size=2)
# for episode in range(500):
#     state = env.reset()
#     for step in range(200):
#         action = agent.act(state)
#         next_state, reward, done, _ = env.step(action)
#         agent.remember(state, action, reward, next_state, done)
#         agent.replay()
#         state = next_state
#     agent.update_target()  # Update target network every episode</div>

<h2>DQN vs Q-Learning</h2>
<table>
<tr><th>Feature</th><th>Q-Learning</th><th>DQN</th></tr>
<tr><td>State representation</td><td>Discrete (table lookup)</td><td>Any (image, continuous)</td></tr>
<tr><td>Scalability</td><td>Small state spaces</td><td>Millions of states</td></tr>
<tr><td>Generalization</td><td>None (exact states only)</td><td>Generalizes to similar states</td></tr>
<tr><td>Stability</td><td>Guaranteed convergence</td><td>Needs replay + target network</td></tr>
<tr><td>Compute</td><td>Very fast</td><td>GPU recommended</td></tr>
</table>

<div class="warning"><p>DQN only works for discrete action spaces (choose from N actions). For continuous actions (like robot joint angles), use DDPG or SAC instead.</p></div>

</div>
</body>
</html>