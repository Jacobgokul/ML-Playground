<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>Reinforcement Learning - ML Playground</title>
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;color:#1e293b;line-height:1.7}
.top-bar{background:#0f172a;padding:14px 32px;display:flex;align-items:center;gap:16px;position:sticky;top:0;z-index:10}
.top-bar a{color:#94a3b8;text-decoration:none;font-size:14px;font-weight:600;transition:.15s}
.top-bar a:hover{color:#f1f5f9}
.top-bar .sep{color:#334155}
.top-bar .current{color:#60a5fa}
.container{max-width:900px;margin:0 auto;padding:40px 32px 80px}
h1{font-size:32px;font-weight:800;letter-spacing:-.8px;margin-bottom:8px;color:#0f172a}
.subtitle{font-size:16px;color:#64748b;margin-bottom:32px;border-bottom:2px solid #e2e8f0;padding-bottom:20px}
h2{font-size:22px;font-weight:700;margin:36px 0 14px;color:#0f172a;letter-spacing:-.3px}
h3{font-size:17px;font-weight:700;margin:24px 0 10px;color:#1e293b}
p{margin-bottom:14px;font-size:15px;color:#334155}
ul,ol{margin:0 0 16px 24px;font-size:15px;color:#334155}
li{margin-bottom:6px}
.card{background:#fff;border:1px solid #e2e8f0;border-radius:12px;padding:24px;margin:20px 0}
.card-title{font-size:14px;font-weight:700;color:#64748b;text-transform:uppercase;letter-spacing:.8px;margin-bottom:12px}
.formula{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:15px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.8}
.code{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:13px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.6}
.highlight{background:#eff6ff;border-left:4px solid #3b82f6;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}
.highlight p{margin:0;color:#1e40af;font-size:14px}
.warning{background:#fef3c7;border-left:4px solid #f59e0b;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}
.warning p{margin:0;color:#92400e;font-size:14px}
.grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:16px 0}
.grid-item{background:#fff;border:1px solid #e2e8f0;border-radius:10px;padding:18px}
.grid-item h4{font-size:14px;font-weight:700;margin-bottom:6px}
.grid-item p{font-size:13px;margin:0}
.tag{display:inline-block;padding:3px 10px;border-radius:20px;font-size:11px;font-weight:600;background:#f1f5f9;color:#475569;margin:2px}
.pros{color:#059669}.cons{color:#dc2626}
table{width:100%;border-collapse:collapse;margin:16px 0;font-size:14px}
th{background:#f1f5f9;padding:10px 14px;text-align:left;font-weight:700;border:1px solid #e2e8f0}
td{padding:10px 14px;border:1px solid #e2e8f0}
@media(max-width:700px){.container{padding:20px 16px 60px}.grid{grid-template-columns:1fr}h1{font-size:24px}}
</style>
</head>
<body>
<div class="top-bar">
<a href="../../index.html">ML Playground</a>
<span class="sep">/</span>
<span class="current">RL Introduction</span>

<a href="https://github.com/Jacobgokul/ML-Playground/blob/main/machine_learning/reinforcement_learning/RL_intro.ipynb" target="_blank" style="color:#94a3b8;text-decoration:none;font-size:13px;font-weight:600;display:flex;align-items:center;gap:6px;margin-left:auto;transition:.15s" onmouseover="this.style.color='#f1f5f9'" onmouseout="this.style.color='#94a3b8'"><svg width="14" height="14" viewBox="0 0 16 16" fill="currentColor" style="vertical-align:-2px"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> View Notebook</a>
</div>
<div class="container">

<h1>Reinforcement Learning (RL)</h1>
<p class="subtitle">A paradigm where an agent learns optimal behavior by interacting with an environment, receiving rewards for good actions and penalties for bad ones.</p>

<h2>What It Is</h2>
<p>Reinforcement Learning is a type of machine learning where an agent learns to make decisions by trial and error. It interacts with an environment, takes actions, observes outcomes, and receives rewards or penalties. The goal is to learn a policy (strategy) that maximizes cumulative reward over time.</p>

<div class="highlight"><p>RL is fundamentally different from supervised learning. There are no labeled examples. The agent must explore and discover which actions yield the best rewards through experience.</p></div>

<h2>Core Components</h2>
<div class="grid">
<div class="grid-item">
<h4>Agent</h4>
<p>The learner/decision-maker (e.g., an AI bot, robot, or game player).</p>
</div>
<div class="grid-item">
<h4>Environment</h4>
<p>The world the agent interacts with (e.g., a game, a maze, a road).</p>
</div>
<div class="grid-item">
<h4>State</h4>
<p>The current situation the agent observes (e.g., position on a grid).</p>
</div>
<div class="grid-item">
<h4>Action</h4>
<p>What the agent does (e.g., move left, accelerate, jump).</p>
</div>
<div class="grid-item">
<h4>Reward</h4>
<p>Feedback signal: positive for good actions, negative for bad ones.</p>
</div>
<div class="grid-item">
<h4>Policy</h4>
<p>The strategy mapping states to actions. The goal is to find the optimal policy.</p>
</div>
</div>

<h2>How It Works</h2>
<div class="card">
<div class="card-title">The RL Loop</div>
<ol>
<li><strong>Observe</strong> the current state of the environment</li>
<li><strong>Choose an action</strong> based on the current policy</li>
<li><strong>Execute the action</strong> &mdash; environment transitions to a new state</li>
<li><strong>Receive a reward</strong> (positive or negative feedback)</li>
<li><strong>Update the policy</strong> based on the experience</li>
<li><strong>Repeat</strong> for many episodes until the agent learns the optimal behavior</li>
</ol>
</div>

<h3>Example: Game AI</h3>
<div class="formula">Car racing game:
  Stay on track  = +1 point
  Go off road    = -5 points
  Complete a lap = +10 points

The AI tries different strategies, observes which actions
lead to higher cumulative reward, and improves over time.</div>

<h2>Types of RL</h2>
<div class="grid">
<div class="grid-item">
<h4>Model-Free</h4>
<p>No knowledge of environment dynamics. Learns purely from experience. Most common in practice. Algorithms: Q-Learning, DQN, SARSA, Policy Gradient.</p>
</div>
<div class="grid-item">
<h4>Model-Based</h4>
<p>Learns or is given a model of the environment. Can plan ahead by simulating future states. Algorithms: Dyna-Q, Monte Carlo Tree Search (AlphaGo).</p>
</div>
</div>

<h2>Popular RL Algorithms</h2>
<table>
<tr><th>Algorithm</th><th>Type</th><th>Key Idea</th></tr>
<tr><td>Q-Learning</td><td>Model-Free, Value-based</td><td>Learns a table of (state, action) values. Classic and foundational.</td></tr>
<tr><td>SARSA</td><td>Model-Free, Value-based</td><td>Like Q-Learning but updates using the action actually taken (on-policy).</td></tr>
<tr><td>DQN</td><td>Model-Free, Value-based</td><td>Q-Learning with neural networks. Handles complex state spaces (images, games).</td></tr>
<tr><td>Policy Gradient</td><td>Model-Free, Policy-based</td><td>Directly learns the policy function instead of value estimates.</td></tr>
<tr><td>Actor-Critic</td><td>Model-Free, Hybrid</td><td>Combines value-based (Critic) and policy-based (Actor) for faster learning.</td></tr>
<tr><td>Monte Carlo</td><td>Model-Free</td><td>Learns from complete episodes by averaging observed rewards.</td></tr>
<tr><td>Dynamic Programming</td><td>Model-Based</td><td>Uses Bellman equations. Requires full knowledge of environment.</td></tr>
</table>

<h2>Exploration vs Exploitation</h2>
<p>The fundamental tradeoff in RL:</p>
<ul>
<li><strong>Exploration</strong> &mdash; Try new actions to discover potentially better rewards</li>
<li><strong>Exploitation</strong> &mdash; Use the best known action to maximize immediate reward</li>
</ul>
<p>The epsilon-greedy strategy balances both: with probability epsilon, explore randomly; otherwise, exploit the best known action.</p>

<h2>Real-World Applications</h2>
<ul>
<li><strong>Games</strong> &mdash; AlphaGo, Chess AI, Atari game-playing agents</li>
<li><strong>Robotics</strong> &mdash; Robot navigation, manipulation, walking</li>
<li><strong>Self-driving cars</strong> &mdash; Learning to navigate roads safely</li>
<li><strong>Finance</strong> &mdash; Automated trading strategies</li>
<li><strong>Healthcare</strong> &mdash; Personalized treatment plans</li>
<li><strong>Recommendations</strong> &mdash; Dynamic content/ad placement</li>
</ul>

<div class="warning"><p>RL requires many episodes of interaction to learn well. Training can be unstable and sample-inefficient. Start with simple environments (like GridWorld) before tackling complex problems.</p></div>

<p><span class="tag">Reinforcement Learning</span> <span class="tag">Agent</span> <span class="tag">Policy</span> <span class="tag">Reward</span></p>

</div>
</body>
</html>