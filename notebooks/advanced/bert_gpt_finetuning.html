<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>BERT / GPT Fine-Tuning - ML Playground</title>
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;color:#1e293b;line-height:1.7}
.top-bar{background:#0f172a;padding:14px 32px;display:flex;align-items:center;gap:16px;position:sticky;top:0;z-index:10}
.top-bar a{color:#94a3b8;text-decoration:none;font-size:14px;font-weight:600;transition:.15s}
.top-bar a:hover{color:#f1f5f9}
.top-bar .sep{color:#334155}
.top-bar .current{color:#60a5fa}
.container{max-width:900px;margin:0 auto;padding:40px 32px 80px}
h1{font-size:32px;font-weight:800;letter-spacing:-.8px;margin-bottom:8px;color:#0f172a}
.subtitle{font-size:16px;color:#64748b;margin-bottom:32px;border-bottom:2px solid #e2e8f0;padding-bottom:20px}
h2{font-size:22px;font-weight:700;margin:36px 0 14px;color:#0f172a;letter-spacing:-.3px}
h3{font-size:17px;font-weight:700;margin:24px 0 10px;color:#1e293b}
p{margin-bottom:14px;font-size:15px;color:#334155}
ul,ol{margin:0 0 16px 24px;font-size:15px;color:#334155}
li{margin-bottom:6px}
.card{background:#fff;border:1px solid #e2e8f0;border-radius:12px;padding:24px;margin:20px 0}
.card-title{font-size:14px;font-weight:700;color:#64748b;text-transform:uppercase;letter-spacing:.8px;margin-bottom:12px}
.formula{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:15px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.8}
.code{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:13px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.6}
.highlight{background:#eff6ff;border-left:4px solid #3b82f6;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}
.highlight p{margin:0;color:#1e40af;font-size:14px}
.warning{background:#fef3c7;border-left:4px solid #f59e0b;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}
.warning p{margin:0;color:#92400e;font-size:14px}
.grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:16px 0}
.grid-item{background:#fff;border:1px solid #e2e8f0;border-radius:10px;padding:18px}
.grid-item h4{font-size:14px;font-weight:700;margin-bottom:6px}
.grid-item p{font-size:13px;margin:0}
.tag{display:inline-block;padding:3px 10px;border-radius:20px;font-size:11px;font-weight:600;background:#f1f5f9;color:#475569;margin:2px}
.pros{color:#059669}.cons{color:#dc2626}
table{width:100%;border-collapse:collapse;margin:16px 0;font-size:14px}
th{background:#f1f5f9;padding:10px 14px;text-align:left;font-weight:700;border:1px solid #e2e8f0}
td{padding:10px 14px;border:1px solid #e2e8f0}
@media(max-width:700px){.container{padding:20px 16px 60px}.grid{grid-template-columns:1fr}h1{font-size:24px}}
</style>
</head>
<body>
<div class="top-bar">
<a href="../../index.html">ML Playground</a>
<span class="sep">/</span>
<span class="current">BERT / GPT Fine-Tuning</span>
</div>
<div class="container">

<h1>BERT / GPT Fine-Tuning</h1>
<p class="subtitle">Take pre-trained language models and adapt them for your specific NLP tasks using HuggingFace Transformers.</p>

<h2>What is Fine-Tuning?</h2>
<p>Fine-tuning takes a pre-trained language model (BERT, GPT, RoBERTa) that has already learned language patterns from billions of words, and further trains it on your specific task with your labeled data. The model keeps its language understanding and just learns the new task on top.</p>

<div class="highlight"><p>BERT was pre-trained on all of English Wikipedia + BookCorpus (3.3 billion words). Fine-tuning lets you leverage all that knowledge with just a few hundred labeled examples.</p></div>

<h2>Pre-Training vs Fine-Tuning</h2>
<div class="formula">PRE-TRAINING (done by Google/OpenAI, takes weeks on 100s of GPUs):
  Task: Masked Language Model (BERT) or Next Token Prediction (GPT)
  Data: Billions of words from the internet
  Result: General language understanding

FINE-TUNING (done by you, takes minutes-hours on 1 GPU):
  Task: Your specific task (sentiment, NER, QA, etc.)
  Data: Your labeled dataset (hundreds to thousands of examples)
  Result: Task-specific model with pre-trained knowledge</div>

<h2>HuggingFace Ecosystem</h2>
<div class="grid">
<div class="grid-item">
<h4>transformers</h4>
<p>Library with 100,000+ pre-trained models. Simple API for loading models and tokenizers.</p>
</div>
<div class="grid-item">
<h4>datasets</h4>
<p>Library with 1000s of ready-to-use datasets. Easy loading, processing, and caching.</p>
</div>
<div class="grid-item">
<h4>Trainer</h4>
<p>High-level training API. Handles training loops, evaluation, saving, logging automatically.</p>
</div>
<div class="grid-item">
<h4>Pipeline</h4>
<p>One-line inference. pipeline("sentiment-analysis")("I love this!") → Positive 0.99</p>
</div>
</div>

<h2>Code: Sentiment Classification with BERT</h2>
<div class="code">from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score

# Load dataset
dataset = load_dataset("imdb")  # 25K train, 25K test movie reviews

# Load pre-trained BERT tokenizer and model
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize data
def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=256)

tokenized = dataset.map(tokenize, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    learning_rate=2e-5,       # Low learning rate for fine-tuning!
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# Metrics
def compute_metrics(eval_pred):
    preds = np.argmax(eval_pred.predictions, axis=1)
    return {"accuracy": accuracy_score(eval_pred.label_ids, preds)}

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    compute_metrics=compute_metrics,
)
trainer.train()

# Inference
from transformers import pipeline
classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
print(classifier("This movie was absolutely fantastic!"))
# → [{'label': 'POSITIVE', 'score': 0.998}]</div>

<h2>Code: Named Entity Recognition (NER)</h2>
<div class="code">from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# Load pre-trained NER model
model_name = "dslim/bert-base-NER"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

ner = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

text = "Elon Musk founded SpaceX in Hawthorne, California in 2002."
entities = ner(text)
for entity in entities:
    print(f"  {entity['word']:20s} → {entity['entity_group']:5s} (score: {entity['score']:.3f})")

# Output:
#   Elon Musk            → PER   (score: 0.998)
#   SpaceX               → ORG   (score: 0.997)
#   Hawthorne            → LOC   (score: 0.993)
#   California           → LOC   (score: 0.999)</div>

<h2>Common NLP Tasks</h2>
<table>
<tr><th>Task</th><th>Model Class</th><th>Example</th></tr>
<tr><td>Text Classification</td><td>AutoModelForSequenceClassification</td><td>Sentiment, spam, topic</td></tr>
<tr><td>Named Entity Recognition</td><td>AutoModelForTokenClassification</td><td>Find names, places, dates</td></tr>
<tr><td>Question Answering</td><td>AutoModelForQuestionAnswering</td><td>Extract answers from context</td></tr>
<tr><td>Summarization</td><td>AutoModelForSeq2SeqLM</td><td>Summarize articles</td></tr>
<tr><td>Translation</td><td>AutoModelForSeq2SeqLM</td><td>English → French</td></tr>
<tr><td>Text Generation</td><td>AutoModelForCausalLM</td><td>Complete text, chat</td></tr>
</table>

<h2>Fine-Tuning Tips</h2>
<ul>
<li><strong>Learning rate</strong>: 2e-5 to 5e-5 (much lower than training from scratch)</li>
<li><strong>Epochs</strong>: 2-4 is usually enough (more can overfit)</li>
<li><strong>Batch size</strong>: 16-32 (limited by GPU memory)</li>
<li><strong>Max length</strong>: BERT max = 512 tokens. Truncate or use a long-context model</li>
<li><strong>Warmup</strong>: Use linear warmup for first 10% of steps</li>
<li><strong>Weight decay</strong>: 0.01 helps prevent overfitting</li>
</ul>

<div class="warning"><p>Fine-tuning BERT-base needs ~11GB GPU RAM. If you're limited, use DistilBERT (40% smaller, 97% accuracy) or use Google Colab's free GPU.</p></div>

</div>
</body>
</html>