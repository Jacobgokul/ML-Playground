<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>Hierarchical Clustering - ML Playground</title>
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;color:#1e293b;line-height:1.7}
.top-bar{background:#0f172a;padding:14px 32px;display:flex;align-items:center;gap:16px;position:sticky;top:0;z-index:10}
.top-bar a{color:#94a3b8;text-decoration:none;font-size:14px;font-weight:600;transition:.15s}
.top-bar a:hover{color:#f1f5f9}
.top-bar .sep{color:#334155}
.top-bar .current{color:#60a5fa}
.container{max-width:900px;margin:0 auto;padding:40px 32px 80px}
h1{font-size:32px;font-weight:800;letter-spacing:-.8px;margin-bottom:8px;color:#0f172a}
.subtitle{font-size:16px;color:#64748b;margin-bottom:32px;border-bottom:2px solid #e2e8f0;padding-bottom:20px}
h2{font-size:22px;font-weight:700;margin:36px 0 14px;color:#0f172a;letter-spacing:-.3px}
h3{font-size:17px;font-weight:700;margin:24px 0 10px;color:#1e293b}
p{margin-bottom:14px;font-size:15px;color:#334155}
ul,ol{margin:0 0 16px 24px;font-size:15px;color:#334155}
li{margin-bottom:6px}
.card{background:#fff;border:1px solid #e2e8f0;border-radius:12px;padding:24px;margin:20px 0}
.card-title{font-size:14px;font-weight:700;color:#64748b;text-transform:uppercase;letter-spacing:.8px;margin-bottom:12px}
.formula{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:15px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.8}
.code{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:13px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.6}
.highlight{background:#eff6ff;border-left:4px solid #3b82f6;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}
.highlight p{margin:0;color:#1e40af;font-size:14px}
.warning{background:#fef3c7;border-left:4px solid #f59e0b;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}
.warning p{margin:0;color:#92400e;font-size:14px}
.grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:16px 0}
.grid-item{background:#fff;border:1px solid #e2e8f0;border-radius:10px;padding:18px}
.grid-item h4{font-size:14px;font-weight:700;margin-bottom:6px}
.grid-item p{font-size:13px;margin:0}
.tag{display:inline-block;padding:3px 10px;border-radius:20px;font-size:11px;font-weight:600;background:#f1f5f9;color:#475569;margin:2px}
.pros{color:#059669}.cons{color:#dc2626}
table{width:100%;border-collapse:collapse;margin:16px 0;font-size:14px}
th{background:#f1f5f9;padding:10px 14px;text-align:left;font-weight:700;border:1px solid #e2e8f0}
td{padding:10px 14px;border:1px solid #e2e8f0}
@media(max-width:700px){.container{padding:20px 16px 60px}.grid{grid-template-columns:1fr}h1{font-size:24px}}
</style>
</head>
<body>
<div class="top-bar">
<a href="../../index.html">ML Playground</a>
<span class="sep">/</span>
<span class="current">Hierarchical Clustering</span>

<a href="https://github.com/Jacobgokul/ML-Playground/blob/main/machine_learning/unsupervised/hierarchical_clustering.ipynb" target="_blank" style="color:#94a3b8;text-decoration:none;font-size:13px;font-weight:600;display:flex;align-items:center;gap:6px;margin-left:auto;transition:.15s" onmouseover="this.style.color='#f1f5f9'" onmouseout="this.style.color='#94a3b8'"><svg width="14" height="14" viewBox="0 0 16 16" fill="currentColor" style="vertical-align:-2px"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> View Notebook</a>
</div>
<div class="container">

<h1>Hierarchical Clustering</h1>
<p class="subtitle">An unsupervised algorithm that builds a tree of clusters (dendrogram) without needing to specify the number of clusters upfront.</p>

<h2>What It Is</h2>
<p>Hierarchical clustering groups similar data points into clusters while building a hierarchy (tree-like structure). Unlike K-Means, you do not need to specify the number of clusters in advance. The output is a dendrogram that you cut at the desired level.</p>

<div class="highlight"><p>Two approaches exist: Agglomerative (bottom-up, most common) starts with each point as its own cluster and merges upward. Divisive (top-down) starts with one cluster and splits downward.</p></div>

<h2>Agglomerative Clustering (Bottom-Up)</h2>
<div class="card">
<div class="card-title">Algorithm Steps</div>
<ol>
<li><strong>Initialize</strong> &mdash; Each data point starts as its own cluster. For points A, B, C, D, E you get {A}, {B}, {C}, {D}, {E}.</li>
<li><strong>Compute distance matrix</strong> &mdash; Calculate pairwise distances between all clusters (Euclidean is common).</li>
<li><strong>Merge closest</strong> &mdash; Find the two clusters with the smallest distance and merge them. E.g., if A and B are closest, merge into {A, B}.</li>
<li><strong>Update distances</strong> &mdash; Recompute distances using a linkage method (see below).</li>
<li><strong>Repeat</strong> &mdash; Keep merging until all points form a single cluster.</li>
<li><strong>Cut the dendrogram</strong> &mdash; Choose a distance threshold to get the desired number of clusters.</li>
</ol>
</div>

<h3>Linkage Methods</h3>
<table>
<tr><th>Method</th><th>Distance Between Clusters</th><th>Behavior</th></tr>
<tr><td>Single Linkage</td><td>Minimum distance between any two points</td><td>Can create elongated, chain-like clusters</td></tr>
<tr><td>Complete Linkage</td><td>Maximum distance between any two points</td><td>Produces compact, spherical clusters</td></tr>
<tr><td>Average Linkage</td><td>Average of all pairwise distances</td><td>Balanced approach</td></tr>
<tr><td>Ward's Method</td><td>Minimizes total within-cluster variance</td><td>Tends to create equal-sized clusters (most popular)</td></tr>
</table>

<h2>Code: Agglomerative Clustering</h2>
<div class="code">import pandas as pd
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import matplotlib.pyplot as plt

data = {
    "Age": [22, 25, 28, 30, 32, 35, 40, 45, 50, 55],
    "Income_LPA": [16, 8, 10, 9, 15, 12, 22, 26, 10, 35],
}

df = pd.DataFrame(data)

# Select and scale features
X = df[["Age", "Income_LPA"]]
X_scaled = StandardScaler().fit_transform(X)

# Perform Agglomerative Clustering (Ward's method)
Z = linkage(X_scaled, method="ward")

# Plot dendrogram
plt.figure(figsize=(10, 5))
dendrogram(Z, truncate_mode="level", p=5)
plt.title("Agglomerative Hierarchical Clustering Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()</div>

<h3>Cut the Dendrogram into Clusters</h3>
<div class="code"># Cut dendrogram into 2 clusters
clusters = fcluster(Z, t=2, criterion="maxclust")
df["Cluster"] = clusters
print(df)

import seaborn as sns
# Visualize clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x="Age", y="Income_LPA", hue="Cluster", palette="Set1", s=100)
plt.title("Agglomerative Clustering on Age vs Income")
plt.show()</div>

<h2>Divisive Clustering (Top-Down)</h2>
<p>The opposite approach: start with all points in one cluster, then recursively split the cluster with the largest variance using a method like K-Means.</p>

<div class="code">import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

data = {
    "Age": [22, 25, 28, 30, 32, 35, 40, 45, 50, 55],
    "Income_LPA": [16, 8, 10, 9, 15, 12, 22, 26, 10, 35]
}
df = pd.DataFrame(data)

# Start with all points in one cluster
df['Cluster'] = 0

# Split the largest cluster into 2 using KMeans
kmeans = KMeans(n_clusters=2, random_state=42)
df['Cluster'] = kmeans.fit_predict(df[['Age', 'Income_LPA']])

plt.figure(figsize=(8, 6))
for cluster in df['Cluster'].unique():
    subset = df[df['Cluster'] == cluster]
    plt.scatter(subset['Age'], subset['Income_LPA'], label=f'Cluster {cluster}')

plt.xlabel("Age")
plt.ylabel("Income (LPA)")
plt.title("Divisive Clustering (Top-Down Example)")
plt.legend()
plt.show()</div>

<h2>When to Use Hierarchical Clustering</h2>
<table>
<tr><th>Good For</th><th>Not Ideal For</th></tr>
<tr><td>You do not know the number of clusters</td><td>Very large datasets (O(n^3) time, O(n^2) memory)</td></tr>
<tr><td>You want to explore cluster structure visually</td><td>High-dimensional data without reduction</td></tr>
<tr><td>Small to medium datasets</td><td>Real-time applications</td></tr>
<tr><td>Taxonomies, gene expression analysis</td><td>When speed matters more than interpretability</td></tr>
</table>

<div class="warning"><p>Hierarchical clustering is computationally expensive. For datasets larger than ~10,000 points, consider K-Means or DBSCAN instead. If you must use it, Ward's linkage is the best default choice.</p></div>

<p><span class="tag">Unsupervised</span> <span class="tag">Clustering</span> <span class="tag">Dendrogram</span> <span class="tag">scipy</span></p>

</div>
</body>
</html>