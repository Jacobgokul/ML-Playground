<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>ML Evaluation Metrics - ML Playground</title>
<style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;color:#1e293b;line-height:1.7}.top-bar{background:#0f172a;padding:14px 32px;display:flex;align-items:center;gap:16px;position:sticky;top:0;z-index:10}.top-bar a{color:#94a3b8;text-decoration:none;font-size:14px;font-weight:600;transition:.15s}.top-bar a:hover{color:#f1f5f9}.top-bar .sep{color:#334155}.top-bar .current{color:#60a5fa}.container{max-width:900px;margin:0 auto;padding:40px 32px 80px}h1{font-size:32px;font-weight:800;letter-spacing:-.8px;margin-bottom:8px;color:#0f172a}.subtitle{font-size:16px;color:#64748b;margin-bottom:32px;border-bottom:2px solid #e2e8f0;padding-bottom:20px}h2{font-size:22px;font-weight:700;margin:36px 0 14px;color:#0f172a;letter-spacing:-.3px}h3{font-size:17px;font-weight:700;margin:24px 0 10px;color:#1e293b}p{margin-bottom:14px;font-size:15px;color:#334155}ul,ol{margin:0 0 16px 24px;font-size:15px;color:#334155}li{margin-bottom:6px}.card{background:#fff;border:1px solid #e2e8f0;border-radius:12px;padding:24px;margin:20px 0}.card-title{font-size:14px;font-weight:700;color:#64748b;text-transform:uppercase;letter-spacing:.8px;margin-bottom:12px}.formula{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:15px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.8}.code{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:13px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.6}.highlight{background:#eff6ff;border-left:4px solid #3b82f6;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}.highlight p{margin:0;color:#1e40af;font-size:14px}.warning{background:#fef3c7;border-left:4px solid #f59e0b;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}.warning p{margin:0;color:#92400e;font-size:14px}.grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:16px 0}.grid-item{background:#fff;border:1px solid #e2e8f0;border-radius:10px;padding:18px}.grid-item h4{font-size:14px;font-weight:700;margin-bottom:6px}.grid-item p{font-size:13px;margin:0}.tag{display:inline-block;padding:3px 10px;border-radius:20px;font-size:11px;font-weight:600;background:#f1f5f9;color:#475569;margin:2px}.pros{color:#059669}.cons{color:#dc2626}table{width:100%;border-collapse:collapse;margin:16px 0;font-size:14px}th{background:#f1f5f9;padding:10px 14px;text-align:left;font-weight:700;border:1px solid #e2e8f0}td{padding:10px 14px;border:1px solid #e2e8f0}@media(max-width:700px){.container{padding:20px 16px 60px}.grid{grid-template-columns:1fr}h1{font-size:24px}}</style>
</head>
<body>
<div class="top-bar">
<a href="../../index.html">ML Playground</a>
<span class="sep">/</span>
<span class="current">Evaluation Metrics</span>

<a href="https://github.com/Jacobgokul/ML-Playground/blob/main/machine_learning/supervised/ml_evaluation_metrics.ipynb" target="_blank" style="color:#94a3b8;text-decoration:none;font-size:13px;font-weight:600;display:flex;align-items:center;gap:6px;margin-left:auto;transition:.15s" onmouseover="this.style.color='#f1f5f9'" onmouseout="this.style.color='#94a3b8'"><svg width="14" height="14" viewBox="0 0 16 16" fill="currentColor" style="vertical-align:-2px"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> View Notebook</a>
</div>
<div class="container">

<h1>ML Evaluation Metrics</h1>
<p class="subtitle">Measuring how well your model performs using the right metric for the right task.</p>

<h2>What are Evaluation Metrics?</h2>
<p>Evaluation metrics measure how well a model's predictions match actual values. Different tasks require different metrics. Using the wrong metric can give a misleading picture of model quality.</p>

<div class="highlight"><p>Always evaluate on unseen test data. Evaluating on training data gives over-optimistic results because the model has already seen that data.</p></div>

<h2>Regression Metrics</h2>
<p>For models that predict continuous numeric values (prices, temperatures, etc.).</p>

<h3>Mean Absolute Error (MAE)</h3>
<p>Average of absolute differences between actual and predicted values. Easy to interpret -- it is in the same unit as the target.</p>
<div class="formula">MAE = (1/n) * sum(|actual_i - predicted_i|)

Lower MAE = better. MAE of 5 means predictions are off by 5 units on average.</div>
<div class="code">from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)
print(f"MAE: {mae}")</div>

<h3>Mean Squared Error (MSE)</h3>
<p>Average of squared differences. Penalizes large errors more heavily than MAE because of the squaring.</p>
<div class="formula">MSE = (1/n) * sum((actual_i - predicted_i)^2)

Lower MSE = better. More sensitive to outliers than MAE.</div>
<div class="code">from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print(f"MSE: {mse}")</div>

<h3>Root Mean Squared Error (RMSE)</h3>
<p>Square root of MSE. Gives the error in the same units as the target, while still penalizing large errors.</p>
<div class="formula">RMSE = sqrt(MSE)

Lower RMSE = better. Combines interpretability of MAE with outlier sensitivity of MSE.</div>
<div class="code">import numpy as np
rmse = np.sqrt(mse)
print(f"RMSE: {rmse}")</div>

<h3>R-squared (R2 Score)</h3>
<p>Measures how much variance in the target the model explains. Ranges from 0 to 1 (can be negative for very bad models).</p>
<div class="formula">R2 = 1 - (SS_res / SS_tot)

Where:
  SS_res = sum((actual - predicted)^2)   # residual sum of squares
  SS_tot = sum((actual - mean)^2)        # total sum of squares

R2 = 1.0 --> perfect fit
R2 = 0.0 --> model is as good as predicting the mean
R2 &lt; 0   --> model is worse than predicting the mean</div>
<div class="code">from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print(f"R2 Score: {r2}")</div>

<h2>Classification Metrics</h2>
<p>For models that predict categories (spam/not spam, disease/healthy, etc.).</p>

<h3>Accuracy</h3>
<p>Percentage of correct predictions. Simple but misleading on imbalanced datasets.</p>
<div class="formula">Accuracy = correct_predictions / total_predictions</div>
<div class="code">from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")</div>

<div class="warning"><p>If 95% of emails are not spam, a model that always predicts "not spam" has 95% accuracy but catches zero spam. Always check precision and recall too.</p></div>

<h3>Confusion Matrix</h3>
<p>A 2x2 table showing all four types of predictions:</p>
<table>
<tr><th></th><th>Predicted Positive</th><th>Predicted Negative</th></tr>
<tr><td><strong>Actually Positive</strong></td><td>True Positive (TP)</td><td>False Negative (FN)</td></tr>
<tr><td><strong>Actually Negative</strong></td><td>False Positive (FP)</td><td>True Negative (TN)</td></tr>
</table>
<div class="code">from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()</div>

<h3>Precision</h3>
<p>Of all positive predictions, how many were actually positive? Important when false positives are costly.</p>
<div class="formula">Precision = TP / (TP + FP)

Example: Of 100 emails flagged as spam, 90 actually were spam --> Precision = 0.90</div>
<div class="code">from sklearn.metrics import precision_score
precision = precision_score(y_test, y_pred)
print(f"Precision: {precision}")</div>

<h3>Recall (Sensitivity)</h3>
<p>Of all actual positives, how many did we correctly identify? Important when false negatives are costly.</p>
<div class="formula">Recall = TP / (TP + FN)

Example: Of 100 actual spam emails, we caught 80 --> Recall = 0.80</div>
<div class="code">from sklearn.metrics import recall_score
recall = recall_score(y_test, y_pred)
print(f"Recall: {recall}")</div>

<h3>F1 Score</h3>
<p>Harmonic mean of precision and recall. Balances both metrics. Use when you need a single number that accounts for both false positives and false negatives.</p>
<div class="formula">F1 = 2 * (Precision * Recall) / (Precision + Recall)

F1 = 1.0 --> perfect precision and recall
F1 = 0.0 --> either precision or recall is 0</div>
<div class="code">from sklearn.metrics import f1_score
f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1}")</div>

<h3>ROC-AUC Score</h3>
<p>Measures the model's ability to distinguish between classes across all thresholds. AUC = 1 means perfect, AUC = 0.5 means random guessing.</p>
<div class="code">from sklearn.metrics import roc_auc_score
# y_pred_prob = model.predict_proba(X_test)[:, 1]  # get probability of positive class
auc = roc_auc_score(y_test, y_pred_prob)
print(f"ROC-AUC Score: {auc}")</div>

<h2>Which Metric to Use?</h2>
<table>
<tr><th>Scenario</th><th>Best Metric</th><th>Why</th></tr>
<tr><td>Regression (predict a number)</td><td>RMSE or R2</td><td>RMSE penalizes large errors; R2 shows explanatory power</td></tr>
<tr><td>Balanced classification</td><td>Accuracy or F1</td><td>Accuracy works when classes are balanced</td></tr>
<tr><td>Imbalanced classification</td><td>F1, Precision, Recall</td><td>Accuracy is misleading with imbalanced classes</td></tr>
<tr><td>Spam detection</td><td>Precision</td><td>Avoid flagging real emails as spam (minimize FP)</td></tr>
<tr><td>Medical diagnosis</td><td>Recall</td><td>Avoid missing actual sick patients (minimize FN)</td></tr>
<tr><td>Ranking / probability models</td><td>ROC-AUC</td><td>Measures performance across all thresholds</td></tr>
</table>

<p><span class="tag">Metrics</span> <span class="tag">Regression</span> <span class="tag">Classification</span> <span class="tag">MAE</span> <span class="tag">RMSE</span> <span class="tag">F1</span> <span class="tag">ROC-AUC</span></p>
</div>
</body>
</html>