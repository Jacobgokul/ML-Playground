<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>XGBoost - ML Playground</title>
<style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;color:#1e293b;line-height:1.7}.top-bar{background:#0f172a;padding:14px 32px;display:flex;align-items:center;gap:16px;position:sticky;top:0;z-index:10}.top-bar a{color:#94a3b8;text-decoration:none;font-size:14px;font-weight:600;transition:.15s}.top-bar a:hover{color:#f1f5f9}.top-bar .sep{color:#334155}.top-bar .current{color:#60a5fa}.container{max-width:900px;margin:0 auto;padding:40px 32px 80px}h1{font-size:32px;font-weight:800;letter-spacing:-.8px;margin-bottom:8px;color:#0f172a}.subtitle{font-size:16px;color:#64748b;margin-bottom:32px;border-bottom:2px solid #e2e8f0;padding-bottom:20px}h2{font-size:22px;font-weight:700;margin:36px 0 14px;color:#0f172a;letter-spacing:-.3px}h3{font-size:17px;font-weight:700;margin:24px 0 10px;color:#1e293b}p{margin-bottom:14px;font-size:15px;color:#334155}ul,ol{margin:0 0 16px 24px;font-size:15px;color:#334155}li{margin-bottom:6px}.card{background:#fff;border:1px solid #e2e8f0;border-radius:12px;padding:24px;margin:20px 0}.card-title{font-size:14px;font-weight:700;color:#64748b;text-transform:uppercase;letter-spacing:.8px;margin-bottom:12px}.formula{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:15px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.8}.code{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:13px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.6}.highlight{background:#eff6ff;border-left:4px solid #3b82f6;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}.highlight p{margin:0;color:#1e40af;font-size:14px}.warning{background:#fef3c7;border-left:4px solid #f59e0b;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}.warning p{margin:0;color:#92400e;font-size:14px}.grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:16px 0}.grid-item{background:#fff;border:1px solid #e2e8f0;border-radius:10px;padding:18px}.grid-item h4{font-size:14px;font-weight:700;margin-bottom:6px}.grid-item p{font-size:13px;margin:0}.tag{display:inline-block;padding:3px 10px;border-radius:20px;font-size:11px;font-weight:600;background:#f1f5f9;color:#475569;margin:2px}.pros{color:#059669}.cons{color:#dc2626}table{width:100%;border-collapse:collapse;margin:16px 0;font-size:14px}th{background:#f1f5f9;padding:10px 14px;text-align:left;font-weight:700;border:1px solid #e2e8f0}td{padding:10px 14px;border:1px solid #e2e8f0}@media(max-width:700px){.container{padding:20px 16px 60px}.grid{grid-template-columns:1fr}h1{font-size:24px}}</style>
</head>
<body>
<div class="top-bar">
<a href="../../index.html">ML Playground</a>
<span class="sep">/</span>
<span class="current">XGBoost</span>

<a href="https://github.com/Jacobgokul/ML-Playground/blob/main/machine_learning/supervised/xgboost_algo.ipynb" target="_blank" style="color:#94a3b8;text-decoration:none;font-size:13px;font-weight:600;display:flex;align-items:center;gap:6px;margin-left:auto;transition:.15s" onmouseover="this.style.color='#f1f5f9'" onmouseout="this.style.color='#94a3b8'"><svg width="14" height="14" viewBox="0 0 16 16" fill="currentColor" style="vertical-align:-2px"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> View Notebook</a>
</div>
<div class="container">

<h1>XGBoost</h1>
<p class="subtitle">Extreme Gradient Boosting -- an optimized, regularized version of gradient boosting that is fast and highly accurate.</p>

<h2>What is XGBoost?</h2>
<p>XGBoost (Extreme Gradient Boosting) is a gradient boosting algorithm with several optimizations that make it faster and more accurate than standard gradient boosting. It builds decision trees sequentially, where each new tree corrects the errors of the previous ones.</p>

<div class="highlight"><p>XGBoost is the most winning algorithm in Kaggle competitions for structured/tabular data. It combines the power of gradient boosting with regularization, parallelization, and smart handling of missing values.</p></div>

<h2>How XGBoost Improves on Gradient Boosting</h2>
<div class="card">
<div class="card-title">XGBoost Optimizations</div>
<ol>
<li><strong>Regularization (L1 + L2)</strong> &mdash; Adds penalties on tree complexity to prevent overfitting</li>
<li><strong>Parallelization</strong> &mdash; Builds trees faster using multiple CPU cores</li>
<li><strong>Missing Value Handling</strong> &mdash; Learns the optimal way to handle missing data automatically</li>
<li><strong>Tree Pruning</strong> &mdash; Grows trees leaf-wise instead of level-wise (more flexible)</li>
<li><strong>Sparsity-aware</strong> &mdash; Efficient with sparse data (like TF-IDF matrices)</li>
<li><strong>Cache optimization</strong> &mdash; Optimized memory access patterns for speed</li>
</ol>
</div>

<h2>Regularization in XGBoost</h2>
<p>Regularization penalizes overly complex models to improve generalization:</p>
<div class="grid">
<div class="grid-item">
<h4>L1 (Lasso) - reg_alpha</h4>
<p>Adds absolute value of weights as penalty. Forces some weights to exactly 0, effectively removing unimportant features. Creates a sparse model.</p>
</div>
<div class="grid-item">
<h4>L2 (Ridge) - reg_lambda</h4>
<p>Adds squared value of weights as penalty. Shrinks large weights but keeps all features. Distributes importance smoothly across features.</p>
</div>
</div>

<div class="formula">L1 (Lasso): penalty = alpha * sum(|weights|)
  --> Some weights become exactly 0 (feature selection)

L2 (Ridge): penalty = lambda * sum(weights^2)
  --> All weights shrink toward 0 but none reach exactly 0</div>

<h2>Code: XGBoost with Regularization Comparison</h2>
<div class="code">import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Dataset
data = {
    "House Size": [500, 800, 1000, 1500, 1800, 2000, 2500, 3000, 3500, 4000],
    "Price": [2000000, 3000000, 4000000, 6000000, 7200000, 8000000,
              10000000, 12000000, 14000000, 16000000]
}
df = pd.DataFrame(data)

X = df[["House Size"]]
y = df["Price"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model with L1 regularization
model_l1 = xgb.XGBRegressor(
    n_estimators=100, learning_rate=0.1, max_depth=3,
    reg_alpha=10, reg_lambda=0, random_state=42
)
model_l1.fit(X_train, y_train)
y_pred_l1 = model_l1.predict(X_test)
print("L1 MSE:", mean_squared_error(y_test, y_pred_l1))

# Model with L2 regularization
model_l2 = xgb.XGBRegressor(
    n_estimators=100, learning_rate=0.1, max_depth=3,
    reg_alpha=0, reg_lambda=10, random_state=42
)
model_l2.fit(X_train, y_train)
y_pred_l2 = model_l2.predict(X_test)
print("L2 MSE:", mean_squared_error(y_test, y_pred_l2))

# Model with no regularization (baseline)
model_none = xgb.XGBRegressor(
    n_estimators=1000, learning_rate=0.5, max_depth=3,
    reg_alpha=0, reg_lambda=0, random_state=42
)
model_none.fit(X_train, y_train)
y_pred_none = model_none.predict(X_test)
print("No Reg MSE:", mean_squared_error(y_test, y_pred_none))</div>

<h2>Key Parameters</h2>
<table>
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>n_estimators</td><td>Number of boosting rounds. More rounds with small learning rate is preferred</td></tr>
<tr><td>learning_rate (eta)</td><td>Step size shrinkage. Typical: 0.01-0.3</td></tr>
<tr><td>max_depth</td><td>Maximum tree depth. Typical: 3-8</td></tr>
<tr><td>reg_alpha</td><td>L1 regularization. Higher = more feature selection</td></tr>
<tr><td>reg_lambda</td><td>L2 regularization. Higher = smoother weights</td></tr>
<tr><td>subsample</td><td>Fraction of samples per tree (stochastic). Typical: 0.6-0.9</td></tr>
<tr><td>colsample_bytree</td><td>Fraction of features per tree. Typical: 0.6-0.9</td></tr>
</table>

<h2>When to Use XGBoost</h2>
<table>
<tr><th>Good For</th><th>Not Ideal For</th></tr>
<tr><td>Structured/tabular data</td><td>Image, audio, or text (use deep learning)</td></tr>
<tr><td>Competition-level accuracy</td><td>Very small datasets</td></tr>
<tr><td>Datasets with missing values</td><td>When simple interpretability is needed</td></tr>
<tr><td>Both classification and regression</td><td>Streaming/online learning</td></tr>
</table>

<div class="warning"><p>XGBoost with no regularization can overfit. Always use reg_alpha and/or reg_lambda. Start with reg_lambda=1 (L2) as a baseline.</p></div>

<p><span class="tag">Ensemble</span> <span class="tag">Boosting</span> <span class="tag">Regularization</span> <span class="tag">Classification</span> <span class="tag">Regression</span></p>
</div>
</body>
</html>