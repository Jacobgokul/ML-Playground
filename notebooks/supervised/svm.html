<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>Support Vector Machine - ML Playground</title>
<style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;color:#1e293b;line-height:1.7}.top-bar{background:#0f172a;padding:14px 32px;display:flex;align-items:center;gap:16px;position:sticky;top:0;z-index:10}.top-bar a{color:#94a3b8;text-decoration:none;font-size:14px;font-weight:600;transition:.15s}.top-bar a:hover{color:#f1f5f9}.top-bar .sep{color:#334155}.top-bar .current{color:#60a5fa}.container{max-width:900px;margin:0 auto;padding:40px 32px 80px}h1{font-size:32px;font-weight:800;letter-spacing:-.8px;margin-bottom:8px;color:#0f172a}.subtitle{font-size:16px;color:#64748b;margin-bottom:32px;border-bottom:2px solid #e2e8f0;padding-bottom:20px}h2{font-size:22px;font-weight:700;margin:36px 0 14px;color:#0f172a;letter-spacing:-.3px}h3{font-size:17px;font-weight:700;margin:24px 0 10px;color:#1e293b}p{margin-bottom:14px;font-size:15px;color:#334155}ul,ol{margin:0 0 16px 24px;font-size:15px;color:#334155}li{margin-bottom:6px}.card{background:#fff;border:1px solid #e2e8f0;border-radius:12px;padding:24px;margin:20px 0}.card-title{font-size:14px;font-weight:700;color:#64748b;text-transform:uppercase;letter-spacing:.8px;margin-bottom:12px}.formula{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:15px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.8}.code{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:13px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.6}.highlight{background:#eff6ff;border-left:4px solid #3b82f6;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}.highlight p{margin:0;color:#1e40af;font-size:14px}.warning{background:#fef3c7;border-left:4px solid #f59e0b;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}.warning p{margin:0;color:#92400e;font-size:14px}.grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:16px 0}.grid-item{background:#fff;border:1px solid #e2e8f0;border-radius:10px;padding:18px}.grid-item h4{font-size:14px;font-weight:700;margin-bottom:6px}.grid-item p{font-size:13px;margin:0}.tag{display:inline-block;padding:3px 10px;border-radius:20px;font-size:11px;font-weight:600;background:#f1f5f9;color:#475569;margin:2px}.pros{color:#059669}.cons{color:#dc2626}table{width:100%;border-collapse:collapse;margin:16px 0;font-size:14px}th{background:#f1f5f9;padding:10px 14px;text-align:left;font-weight:700;border:1px solid #e2e8f0}td{padding:10px 14px;border:1px solid #e2e8f0}@media(max-width:700px){.container{padding:20px 16px 60px}.grid{grid-template-columns:1fr}h1{font-size:24px}}</style>
</head>
<body>
<div class="top-bar">
<a href="../../index.html">ML Playground</a>
<span class="sep">/</span>
<span class="current">SVM</span>

<a href="https://github.com/Jacobgokul/ML-Playground/blob/main/machine_learning/supervised/svm.ipynb" target="_blank" style="color:#94a3b8;text-decoration:none;font-size:13px;font-weight:600;display:flex;align-items:center;gap:6px;margin-left:auto;transition:.15s" onmouseover="this.style.color='#f1f5f9'" onmouseout="this.style.color='#94a3b8'"><svg width="14" height="14" viewBox="0 0 16 16" fill="currentColor" style="vertical-align:-2px"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> View Notebook</a>
</div>
<div class="container">

<h1>Support Vector Machine (SVM)</h1>
<p class="subtitle">A powerful algorithm that finds the optimal hyperplane to separate data into classes with maximum margin.</p>

<h2>What is SVM?</h2>
<p>SVM is a supervised learning algorithm that finds the hyperplane (decision boundary) that best separates data points into different classes. It maximizes the margin between the closest points of each class (called support vectors) and the boundary.</p>

<div class="highlight"><p>SVM works by finding the widest possible "street" between two classes. The edges of the street are defined by the support vectors -- the data points closest to the boundary.</p></div>

<h2>Key Concepts</h2>
<ul>
<li><strong>Hyperplane</strong> &mdash; The decision boundary that separates classes</li>
<li><strong>Support Vectors</strong> &mdash; Data points closest to the hyperplane that define its position</li>
<li><strong>Margin</strong> &mdash; The distance between the hyperplane and nearest support vectors (SVM maximizes this)</li>
<li><strong>Kernel Trick</strong> &mdash; Transforms non-linearly separable data into higher dimensions where a linear boundary works</li>
</ul>

<h2>Kernel Types</h2>
<div class="grid">
<div class="grid-item">
<h4>Linear</h4>
<p>For linearly separable data. Fastest. Use when data can be separated by a straight line/plane.</p>
</div>
<div class="grid-item">
<h4>RBF (Radial Basis Function)</h4>
<p>Default kernel. Maps data to infinite-dimensional space. Works well for most non-linear problems.</p>
</div>
<div class="grid-item">
<h4>Polynomial</h4>
<p>Maps data using polynomial features. Good for data with polynomial relationships.</p>
</div>
<div class="grid-item">
<h4>Sigmoid</h4>
<p>Similar to a neural network activation. Rarely used in practice.</p>
</div>
</div>

<h2>SVR (Support Vector Regression)</h2>
<p>For regression tasks, SVR fits a hyperplane within an epsilon-insensitive tube around the data:</p>
<ul>
<li>Points inside the tube incur no penalty (treated as correct)</li>
<li>Points outside the tube incur a loss</li>
<li>Only support vectors (points on or outside the tube) define the model</li>
</ul>

<h2>Code: SVR for Rent Prediction</h2>
<div class="code">import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Dataset
data = {
    "Size_sqft": [500, 700, 900, 1100, 1500, 1800, 2100],
    "Bedrooms": [1, 1, 2, 2, 3, 3, 4],
    "Rent": [12, 15, 20, 25, 30, 35, 50]  # Rent in thousands
}
df = pd.DataFrame(data)

# Features and target
X = df[["Size_sqft", "Bedrooms"]]
y = df["Rent"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (critical for SVM)
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()

# Train SVR with RBF kernel
svr = SVR(kernel="rbf", C=100, epsilon=0.01)
svr.fit(X_train_scaled, y_train_scaled)

# Predictions (inverse transform to original scale)
y_pred_scaled = svr.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()

# Evaluate
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"R2 Score: {r2:.2f}")</div>

<div class="warning"><p>SVM is very sensitive to feature scales. Always use StandardScaler before training. Also scale the target variable for SVR.</p></div>

<h2>Key Parameters</h2>
<table>
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>C</td><td>Regularization. Higher C = less tolerance for misclassification (risk of overfitting)</td></tr>
<tr><td>kernel</td><td>"linear", "rbf", "poly", "sigmoid". Default is "rbf"</td></tr>
<tr><td>epsilon (SVR)</td><td>Width of the insensitive tube. Larger = more tolerance for errors</td></tr>
<tr><td>gamma</td><td>Controls influence of a single training example. "scale" or "auto" are common defaults</td></tr>
</table>

<h2>When to Use SVM</h2>
<table>
<tr><th>Good For</th><th>Not Ideal For</th></tr>
<tr><td>High-dimensional data (text, genomics)</td><td>Very large datasets (slow training)</td></tr>
<tr><td>Clear margin of separation</td><td>Noisy data with overlapping classes</td></tr>
<tr><td>Binary and multi-class classification</td><td>When you need probability estimates (use SVC with probability=True)</td></tr>
<tr><td>Small to medium datasets</td><td>When interpretability is important</td></tr>
</table>

<p><span class="tag">Classification</span> <span class="tag">Regression</span> <span class="tag">Supervised</span> <span class="tag">Kernel</span> <span class="tag">Margin</span></p>
</div>
</body>
</html>