<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>Random Forest - ML Playground</title>
<style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#f8fafc;color:#1e293b;line-height:1.7}.top-bar{background:#0f172a;padding:14px 32px;display:flex;align-items:center;gap:16px;position:sticky;top:0;z-index:10}.top-bar a{color:#94a3b8;text-decoration:none;font-size:14px;font-weight:600;transition:.15s}.top-bar a:hover{color:#f1f5f9}.top-bar .sep{color:#334155}.top-bar .current{color:#60a5fa}.container{max-width:900px;margin:0 auto;padding:40px 32px 80px}h1{font-size:32px;font-weight:800;letter-spacing:-.8px;margin-bottom:8px;color:#0f172a}.subtitle{font-size:16px;color:#64748b;margin-bottom:32px;border-bottom:2px solid #e2e8f0;padding-bottom:20px}h2{font-size:22px;font-weight:700;margin:36px 0 14px;color:#0f172a;letter-spacing:-.3px}h3{font-size:17px;font-weight:700;margin:24px 0 10px;color:#1e293b}p{margin-bottom:14px;font-size:15px;color:#334155}ul,ol{margin:0 0 16px 24px;font-size:15px;color:#334155}li{margin-bottom:6px}.card{background:#fff;border:1px solid #e2e8f0;border-radius:12px;padding:24px;margin:20px 0}.card-title{font-size:14px;font-weight:700;color:#64748b;text-transform:uppercase;letter-spacing:.8px;margin-bottom:12px}.formula{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:15px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.8}.code{background:#0f172a;color:#e2e8f0;padding:20px 24px;border-radius:10px;font-family:'Courier New',monospace;font-size:13px;margin:16px 0;overflow-x:auto;white-space:pre;line-height:1.6}.highlight{background:#eff6ff;border-left:4px solid #3b82f6;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}.highlight p{margin:0;color:#1e40af;font-size:14px}.warning{background:#fef3c7;border-left:4px solid #f59e0b;padding:16px 20px;border-radius:0 8px 8px 0;margin:16px 0}.warning p{margin:0;color:#92400e;font-size:14px}.grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:16px 0}.grid-item{background:#fff;border:1px solid #e2e8f0;border-radius:10px;padding:18px}.grid-item h4{font-size:14px;font-weight:700;margin-bottom:6px}.grid-item p{font-size:13px;margin:0}.tag{display:inline-block;padding:3px 10px;border-radius:20px;font-size:11px;font-weight:600;background:#f1f5f9;color:#475569;margin:2px}.pros{color:#059669}.cons{color:#dc2626}table{width:100%;border-collapse:collapse;margin:16px 0;font-size:14px}th{background:#f1f5f9;padding:10px 14px;text-align:left;font-weight:700;border:1px solid #e2e8f0}td{padding:10px 14px;border:1px solid #e2e8f0}@media(max-width:700px){.container{padding:20px 16px 60px}.grid{grid-template-columns:1fr}h1{font-size:24px}}</style>
</head>
<body>
<div class="top-bar">
<a href="../../index.html">ML Playground</a>
<span class="sep">/</span>
<span class="current">Random Forest</span>

<a href="https://github.com/Jacobgokul/ML-Playground/blob/main/machine_learning/supervised/random_forest.ipynb" target="_blank" style="color:#94a3b8;text-decoration:none;font-size:13px;font-weight:600;display:flex;align-items:center;gap:6px;margin-left:auto;transition:.15s" onmouseover="this.style.color='#f1f5f9'" onmouseout="this.style.color='#94a3b8'"><svg width="14" height="14" viewBox="0 0 16 16" fill="currentColor" style="vertical-align:-2px"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> View Notebook</a>
</div>
<div class="container">

<h1>Random Forest</h1>
<p class="subtitle">An ensemble of decision trees that combines their predictions for better accuracy and reduced overfitting.</p>

<h2>What is Random Forest?</h2>
<p>Random Forest is an ensemble learning technique that builds multiple decision trees and combines their outputs. Instead of relying on a single tree, it takes the majority vote (classification) or average prediction (regression) from many trees. This reduces overfitting and improves accuracy.</p>

<div class="highlight"><p>Random Forest = Bagging (Bootstrap Aggregating) + Random Feature Selection. Each tree is trained on a random subset of data AND a random subset of features, making every tree unique.</p></div>

<h2>How It Works</h2>
<div class="card">
<div class="card-title">Algorithm Steps</div>
<ol>
<li><strong>Create N bootstrap samples</strong> (random subsets of training data, with replacement)</li>
<li><strong>Train a decision tree</strong> on each sample, but at each split only consider a random subset of features</li>
<li><strong>For classification:</strong> each tree votes, majority wins</li>
<li><strong>For regression:</strong> take the average of all tree predictions</li>
</ol>
</div>

<h2>Key Concepts</h2>
<ul>
<li><strong>Bagging:</strong> Each tree trains on a different random subset of data (reduces variance)</li>
<li><strong>Feature Randomness:</strong> Each tree only considers a random subset of features at each split (reduces correlation between trees)</li>
<li><strong>Aggregation:</strong> Combine all trees for the final prediction</li>
</ul>

<h2>Code: Predicting House Rent (Simple)</h2>
<div class="code">import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Dataset
data = {
    "Size_sqft": [500, 700, 800, 900, 1200, 1500, 1800, 2000, 2200, 2500],
    "Bedrooms": [1, 1, 2, 2, 3, 3, 3, 4, 4, 5],
    "Rent": [5000, 7000, 8500, 9000, 12000, 15000, 18000, 20000, 22000, 25000]
}
df = pd.DataFrame(data)

# Features and target
X = df[["Size_sqft", "Bedrooms"]]
y = df["Rent"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest
rf = RandomForestRegressor(n_estimators=1500)
rf.fit(X_train, y_train)

# Predict and evaluate
y_pred = rf.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R2 Score: {r2:.2f}")</div>

<h2>Code: With Categorical Features</h2>
<div class="code">import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Dataset with more features including categorical
data = {
    "Size_sqft": [500, 700, 800, 900, 1200, 1500, 1800, 2000, 2200, 2500],
    "Bedrooms": [1, 1, 2, 2, 3, 3, 3, 4, 4, 5],
    "Bathrooms": [1, 1, 1, 2, 2, 2, 3, 3, 3, 4],
    "Floor": [1, 2, 3, 1, 5, 3, 6, 10, 12, 15],
    "City": ["Chennai", "Bangalore", "Mumbai", "Chennai", "Delhi",
             "Mumbai", "Bangalore", "Delhi", "Chennai", "Mumbai"],
    "Furnished": [0, 1, 0, 1, 1, 1, 0, 1, 0, 1],
    "Parking_Spots": [0, 1, 1, 2, 2, 1, 2, 3, 2, 3],
    "Rent": [5000, 7000, 8500, 9000, 12000, 15000, 18000, 20000, 22000, 25000]
}
df = pd.DataFrame(data)

X = df.drop("Rent", axis=1)
y = df["Rent"]

# One-hot encode categorical columns
categorical_features = ["City"]
encoder = ColumnTransformer(
    transformers=[("encoded", OneHotEncoder(drop="first"), categorical_features)],
    remainder="passthrough"
)
X_encoded = encoder.fit_transform(X)
feature_names = encoder.get_feature_names_out()
X_encoded_df = pd.DataFrame(X_encoded, columns=feature_names)

# Train and evaluate
X_train, X_test, y_train, y_test = train_test_split(X_encoded_df, y, test_size=0.2, random_state=42)
rf = RandomForestRegressor(n_estimators=50)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

print(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"R2 Score: {r2_score(y_test, y_pred):.2f}")</div>

<h2>Key Parameters</h2>
<table>
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>n_estimators</td><td>Number of trees in the forest. More trees = better accuracy, slower training</td></tr>
<tr><td>max_depth</td><td>Maximum depth of each tree. Limits overfitting</td></tr>
<tr><td>max_features</td><td>Number of features to consider at each split ("sqrt", "log2", or int)</td></tr>
<tr><td>min_samples_split</td><td>Minimum samples needed to split a node</td></tr>
</table>

<h2>When to Use Random Forest</h2>
<table>
<tr><th>Good For</th><th>Not Ideal For</th></tr>
<tr><td>Better accuracy than single decision tree</td><td>When interpretability is critical</td></tr>
<tr><td>Large and complex datasets</td><td>Very high-dimensional sparse data</td></tr>
<tr><td>Reducing overfitting</td><td>Real-time predictions (slower than single tree)</td></tr>
<tr><td>Both classification and regression</td><td>When you need feature importance explanations</td></tr>
</table>

<div class="warning"><p>More trees (n_estimators) generally means better performance, but with diminishing returns. Beyond ~100-500 trees, improvement is minimal while training time increases linearly.</p></div>

<p><span class="tag">Ensemble</span> <span class="tag">Bagging</span> <span class="tag">Classification</span> <span class="tag">Regression</span> <span class="tag">Supervised</span></p>
</div>
</body>
</html>